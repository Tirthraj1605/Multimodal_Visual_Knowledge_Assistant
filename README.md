# ğŸ§  Multimodal_Visual_Knowledge_Assistant

**Multimodal_Visual_Knowledge_Assistant** is a smart AI system that uses computer vision and natural language processing to understand images and generate meaningful text-based responses. By combining the power of **CLIP** and **GPT-2**, this project analyzes images across various domainsâ€”Medical, Fashion, Microscopy, and Natureâ€”and produces relevant insights or creative descriptions.

# OUTPUT Demo 

![Multimodal_Visual_Knowledge_Assistant Screenshot](Output-1.png)
![Multimodal_Visual_Knowledge_Assistant Screenshot](Output-2.png)

---

## ğŸš€ Features

- ğŸ” **Image Classification** using OpenAIâ€™s CLIP model.
- ğŸ§  **Text Generation** using GPT-2 for dynamic, context-aware language.
- ğŸ“· Visual display of images, predictions, and generated texts.
- ğŸ”„ Supports domain-specific prompts for deeper semantic relevance.
- ğŸ’¡ Easy to scale by adding new categories and label sets.

---

## ğŸ“ Project Structure

